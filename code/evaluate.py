from transformers import BertTokenizer, BertForSequenceClassification
import torch
import torch.nn as nn
from modules.training_loop import TrainingLoop
from modules.tokenize_dataset import TokenizeDataset
from modules.activation_tracker import LayerTracker, NeuronTracker
from modules.resource_logging_callback import ResourceLoggingCallback
from modules.set_seed import set_seed
from modules.threshold import Threshold
from copy import deepcopy
from torch.utils.data import DataLoader

from sklearn.metrics import accuracy_score, f1_score, precision_score
import json
from tqdm import tqdm
from matplotlib import pyplot as plt   
import matplotlib
import numpy as np
import seaborn as sns

set_seed(42)

def create_optimizer(model, config):
    optimizer_type = config["optimizer"]["type"]
    lr = config["optimizer"]["lr"]
    weight_decay = config["optimizer"].get("weight_decay", 0.0)

    if optimizer_type == "AdamW":
        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type == "SGD":
        momentum = config["optimizer"].get("momentum", 0.9)
        return torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)
    
def compute_metrics(logits, labels):
    predictions = torch.argmax(logits, dim=-1).cpu().numpy()
    labels = labels.cpu().numpy()
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted")
    p = precision_score(labels, predictions, average="micro")
    return {"accuracy": acc, "f1": f1, "precision": p}


def apply_pruning_to_model(model, neuron_tracker):
    device = next(model.parameters()).device  # ëª¨ë¸ì´ ìˆëŠ” ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    neurons_pruned = []
    for i in range(4):  # ì´ 4ê°œ Layer
        layer_name = f"bert.encoder.layer.{i}.intermediate.dense"

        if layer_name in neuron_tracker.pruning_masks:
            pruning_mask = torch.tensor(neuron_tracker.pruning_masks[layer_name], dtype=torch.float32).to(device)  # ëª¨ë¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            active_neurons = int(pruning_mask.sum().item())  # ì‚´ì•„ë‚¨ì€ ë‰´ëŸ° ê°œìˆ˜

            # ìµœì†Œ 1ê°œ ë‰´ëŸ° ìœ ì§€ (ëª¨ë‘ ì œê±°ë˜ëŠ” ê²½ìš° ë°©ì§€)
            if active_neurons == 0:
                active_neurons = 1
                pruning_mask[0] = 1  # ì²« ë²ˆì§¸ ë‰´ëŸ° ìœ ì§€

            # 1ï¸âƒ£ intermediate.dense ì—…ë°ì´íŠ¸
            layer = dict(model.named_modules())[layer_name]  # ê¸°ì¡´ Layer ê°€ì ¸ì˜¤ê¸°
            with torch.no_grad():
                pruned_weight = layer.weight[pruning_mask.bool(), :].to(device)  # CUDAë¡œ ì´ë™
                pruned_bias = layer.bias[pruning_mask.bool()].to(device) if layer.bias is not None else None  # Biasë„ CUDAë¡œ ì´ë™
                
                # ìƒˆë¡œìš´ Pruned Linear Layer ìƒì„±
                new_layer = nn.Linear(layer.in_features, active_neurons, bias=(layer.bias is not None)).to(device)  # CUDAë¡œ ì´ë™
                new_layer.weight = nn.Parameter(pruned_weight)
                if pruned_bias is not None:
                    new_layer.bias = nn.Parameter(pruned_bias)

            # Pruned Layerë¥¼ ëª¨ë¸ì— ì ìš©
            parts = layer_name.split(".")
            submodule_name = parts[-1]
            parent_module = model
            for part in parts[:-1]:  
                parent_module = getattr(parent_module, part)
            setattr(parent_module, submodule_name, new_layer)

            print(f"Layer {i}: {layer.out_features} to {active_neurons} neurons")

            # 2ï¸âƒ£ output.dense ì—…ë°ì´íŠ¸ (intermediate.denseì™€ ì—°ê²°ëœ ë ˆì´ì–´)
            parts[4] = 'output'  # `intermediate` â†’ `output`ìœ¼ë¡œ ë³€ê²½
            layer_name = ".".join(parts)  # ìƒˆë¡œìš´ ë ˆì´ì–´ ì´ë¦„ ìƒì„±
            
            if layer_name in dict(model.named_modules()):  # `output.dense`ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                output_layer = dict(model.named_modules())[layer_name]
                
                with torch.no_grad():
                    original_weight = output_layer.weight.detach().cpu().numpy()
                    original_bias = output_layer.bias.detach().cpu().numpy() if output_layer.bias is not None else None

                    # output.denseì˜ ì…ë ¥ ë‰´ëŸ° ê°œìˆ˜ ë³€ê²½
                    pruned_weight = torch.tensor(original_weight[:, :active_neurons], dtype=torch.float32).to(device)  # CUDAë¡œ ì´ë™
                    pruned_bias = torch.tensor(original_bias, dtype=torch.float32).to(device) if original_bias is not None else None  # CUDA ì´ë™

                    # ìƒˆë¡œìš´ Pruned Linear Layer ìƒì„±
                    new_output_layer = nn.Linear(active_neurons, output_layer.out_features, bias=(output_layer.bias is not None)).to(device)
                    new_output_layer.weight = nn.Parameter(pruned_weight)
                    if pruned_bias is not None:
                        new_output_layer.bias = nn.Parameter(pruned_bias)

                # Pruned Layerë¥¼ ëª¨ë¸ì— ì ìš©
                parts = layer_name.split(".")
                submodule_name = parts[-1]
                parent_module = model
                for part in parts[:-1]:
                    parent_module = getattr(parent_module, part)
                setattr(parent_module, submodule_name, new_output_layer)
                neurons_pruned.append(2048 - active_neurons)
    return neurons_pruned
                # print(f"Layer {i}: intermediate dense to output dense: ({layer.out_features} {active_neurons} â†’ {output_layer.out_features}) neurons")

            
            
            

def fine_tune_and_evaluate(config, callbacks=None):
    tokenizer = BertTokenizer.from_pretrained(config["model_path"])
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ë‹¤ì–‘í•œ Threshold ì¡°í•© í…ŒìŠ¤íŠ¸
    threshold_configs = {
        # "base": [0, 0, 0, 0],  # ê¸°ë³¸ê°’ (Pruning ì—†ìŒ)
        "[0.8,0.4, 0.4, 0.4]": [0.8, 0.4, 0.4, 0.4],
        "[0.4, 0.8, 0.4, 0.4]": [0.4, 0.8, 0.4, 0.4],
        "[0.4, 0.4, 0.8, 0.4]": [0.4, 0.4, 0.8, 0.4],
        "[0.4, 0.4, 0.4, 0.8]": [0.4, 0.4, 0.4, 0.8],
        "[0.8, 0.8, 0.8, 0.8]": [0.8, 0.8, 0.8, 0.8],
        "[0, 0, 0.6, 0]": [0, 0, 0.6, 0],
        "[0, 0, 0.4, 0]": [0, 0, 0.4, 0],
        "[0, 0, 0.2, 0]": [0, 0, 0.2, 0],
        "[0, 0.6, 0.8, 0]": [0, 0.6, 0.8, 0],
        "[0, 0.4, 0.6, 0]": [0, 0.4, 0.6, 0],
        "[0, 0.2, 0.4, 0]": [0, 0.2, 0.4, 0],
        # "0th": [0.8, 0, 0, 0],
        # "1st": [0, 0.8, 0, 0],
        # "2nd": [0, 0, 0.8, 0],
        # "3rd": [0, 0, 0, 0.8],

        # # ê· ë“±í•œ pruning (ëª¨ë“  Layer ë™ì¼)
        # "uniform": [0.12, 0.13, 0.14, 0.15],  

        # # ì´ˆë°˜ pruning ì¦ê°€ (ì´ˆê¸° Layerë¥¼ ë” ë§ì´ Pruning)
        # "early_more": [0.15, 0.14, 0.13, 0.12],  
        
        # # í›„ë°˜ pruning ì¦ê°€ (í›„ë°˜ Layerë¥¼ ë” ë§ì´ Pruning)
        # "late_more": [0.12, 0.15, 0.22, 0.28],  

        # # ì ì‘í˜• pruning (Layerë³„ ë³€í™”ëŸ‰ ê³ ë ¤)
        # "adaptive_1": [0.14, 0.12, 0.16, 0.20],  
        # "adaptive_2": [0.13, 0.14, 0.20, 0.25],  

        # # ê°€ì¥ ê°•ë ¥í•œ pruning (90th percentile ê·¼ì²˜)
        # "aggressive": [0.10, 0.15, 0.25, 0.34],  

        # # **Linear Scaling (ì„ í˜• ì¦ê°€)**
        # "linear_1": [0.10, 0.15, 0.20, 0.25],  # Layerë³„ ì ì§„ì  ì¦ê°€
        # "linear_2": [0.08, 0.12, 0.16, 0.20],  

        # # **Focused Pruning (íŠ¹ì • Layer ì§‘ì¤‘)**
        # "high_start": [0.25, 0.18, 0.12, 0.10],  # ì´ˆë°˜ Layer ê°•í•œ Pruning
        # "high_end": [0.08, 0.10, 0.15, 0.25],  # í›„ë°˜ Layer ê°•í•œ Pruning

        # # **Exponential Pruning (ì§€ìˆ˜ì  ê°ì†Œ)**
        # "exp_1": [0.20, 0.18, 0.12, 0.08],  # ì´ˆë°˜ Pruning ê°•í•˜ê²Œ
        # "exp_2": [0.05, 0.10, 0.20, 0.30],  # í›„ë°˜ Pruning ê°•í•˜ê²Œ (ì§€ìˆ˜ì  ì¦ê°€)

        # # **ë°±ë¶„ìœ„ìˆ˜ ê¸°ë°˜ pruning**
        # "percentile_85": [0.740, 0.769, 0.790, 0.686],  # 85th percentile
        # "percentile_90": [0.804, 0.847, 0.856, 0.731],  # 90th percentile
        # "percentile_95": [0.914, 0.972, 0.973, 0.811],  # 95th percentile

        # # **ë” ê°•ë ¥í•œ Pruning ì¶”ê°€**
        # "ultra_aggressive": [0.20, 0.30, 0.40, 0.50],  # 95th percentile ì´ìƒì—ì„œ pruning
        # "extreme": [0.25, 0.40, 0.55, 0.70],  # ê±°ì˜ ëª¨ë“  ë‰´ëŸ° ì œê±°
        # "hard_exp": [0.10, 0.20, 0.40, 0.80],  # ì§€ìˆ˜ì  pruning (í›„ë°˜ë¶€ì—ì„œ ê±°ì˜ ì‚­ì œë¨)
        # "percentile_98": [1.10, 1.15, 1.20, 1.25],  # 98th Percentile ê¸°ë°˜ pruning
    }

    results = {}  # ì„±ëŠ¥ ê²°ê³¼ ì €ì¥

    for config_name, threshold in threshold_configs.items():
        print(f"\nRunning with pruning threshold: {config_name} ({threshold})")

        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        model = BertForSequenceClassification.from_pretrained(
            config["model_path"],
            num_labels=2,
            output_hidden_states=True,
            output_attentions=True,
        )
        model.to(device)

        # print("Model Architecture:", model)
        trainer = TrainingLoop(callbacks=callbacks)
        activation_tracker = LayerTracker()
        activation_tracker.register_all_layers(model)
        neuron_tracker = NeuronTracker()
        neuron_tracker.register_all_layers(model)

        glue_dataset = TokenizeDataset(tokenizer, config["task_name"], config["batch_size"], dataset_name="glue")
        train_loader, eval_loader = glue_dataset.glue_data_loader()

        # print("Running 1st Forward Pass for Neuron Pruning...")
        model.eval()

        # 1st Forward Pass (í˜„ì¬ í™œì„±í™” ê°’ ì €ì¥)
        for batch in train_loader:
            with torch.no_grad():
                batch = {k: v.to(device) for k, v in batch.items() if k in ["input_ids", "attention_mask", "token_type_ids"]}
                _ = model(**batch)
            break  # 1íšŒë§Œ ì‹¤í–‰

        # print("Running 2nd Forward Pass to store previous activations...")
        for batch in train_loader:
            with torch.no_grad():
                batch = {k: v.to(device) for k, v in batch.items() if k in ["input_ids", "attention_mask", "token_type_ids"]}
                _ = model(**batch)
            break  # 1íšŒë§Œ ì‹¤í–‰

        # Pruning Mask ìƒì„±
        # Pruning Mask ìƒì„± ë° Layerë³„ ë°±ë¶„ìœ„ìˆ˜ ë¶„ì„
        pruning_mask = []
        layerwise_changes = {}  # Layerë³„ ë³€í™”ëŸ‰ ì €ì¥

        for i in range(4):
            layer_name = f"bert.encoder.layer.{i}.intermediate.dense"
            if layer_name not in activation_tracker.activations:
                # print(f"âš  Warning: {layer_name} activation is missing!")
                continue

            abs_change = neuron_tracker.calculate_neuron_changes(layer_name)
            if abs_change is None:
                # print(f"âš  Warning: {layer_name} has no activation data. Skipping pruning.")
                pruning_mask.append(None)
                continue

            # Layerë³„ ë³€í™”ëŸ‰ ì €ì¥
            layerwise_changes[layer_name] = abs_change.flatten()

            # Pruning Mask ìƒì„±
            pruning_mask.append(neuron_tracker.generate_pruning_mask(layer_name, threshold[i]))
            # print(f"Layer {i}: Pruning Mask ìƒì„± ì™„ë£Œ!")

        # # Layerë³„ ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        # percentiles = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 99]

        # # print("\n**Layer-wise Absolute Change Percentiles:**")
        # for layer_name, changes in layerwise_changes.items():
        #     if len(changes) > 0:
        #         changes = np.array(changes)  # NumPy ë°°ì—´ ë³€í™˜
        #         percentile_values = np.percentile(changes, percentiles)

        #         # print(f"\nğŸ“Œ Layer: {layer_name}")
        #         for p, v in zip(percentiles, percentile_values):
        #             # print(f"{p}th percentile: {v:.6f}")

        #     pruning_mask.append(neuron_tracker.generate_pruning_mask(layer_name, threshold[i]))
        #     # print(f"Layer {i}: Pruning Mask ìƒì„± ì™„ë£Œ!")

        # Pruning ì ìš©
        neuron_pruned = apply_pruning_to_model(model, neuron_tracker)

        # print("Running Fine-tuning...")
        # print(model)
        model.train()
        optimizer = create_optimizer(model, config)
        trainer._trigger_event("on_train_begin")

        train_loss = 0
        for epoch in range(config["num_epochs"]):
            trainer.running_loss = 0.0
            for batch in tqdm(train_loader, desc="Training", leave=False):
                batch = {k: v.to(device) for k, v in batch.items()}
                loss = trainer.forward_pass(model, batch, device)
                trainer.backward_pass(loss, optimizer)

            epoch_loss = trainer.running_loss / len(train_loader)
            print(f"Epoch {epoch + 1} Loss: {epoch_loss:.4f}")
            metrics = trainer.evaluation(model, eval_loader, device, compute_metrics)
            print(f"Epoch {epoch + 1} Metrics: {metrics}")

        trainer._trigger_event("on_train_end")

        # ê²°ê³¼ ì €ì¥
        results[config_name] = {
            
            "threshold": threshold,
            # "train_loss": epoch_loss,
            "accuracy": metrics["accuracy"],
            "f1": metrics["f1"],
            "neurons_pruned": neuron_pruned, 
            # "precision": metrics["precision"],
        }
        
    #   method, threshold, l0, l1, l2, l3, training time, allocated gpu mem, cached gpu memory, accuracy, f1-score
        

    # ëª¨ë“  ê²°ê³¼ ì •ë ¬ í›„ ì¶œë ¥
    sorted_results = sorted(results.items(), key=lambda x: x[1]["accuracy"], reverse=True)
    print("\n**Final Results (Sorted by Accuracy)**:")
    for name, res in sorted_results:
        print(f"{name}: Accuracy={res['accuracy']:.4f}, F1={res['f1']:.4f}, Pruning={res['threshold']}, Neurons Pruned={res['neurons_pruned']}")

    return activation_tracker, neuron_tracker

if __name__ == "__main__":
    with open("../config/evaluate_config.json", "r") as f:
        config = json.load(f)
    
    activation_tracker, neuron_tracker = fine_tune_and_evaluate(config, callbacks=[ResourceLoggingCallback()])
