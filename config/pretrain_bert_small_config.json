{
    "vocab_size": 30522,
    "hidden_size": 512,
    "num_hidden_layers": 4,
    "num_attention_heads": 8,
    "intermediate_size": 2048,
    "max_position_embeddings": 512,
    "hidden_dropout_prob": 0.2,
    "attention_probs_dropout_prob": 0.1,
    "data_collator_config": {
        "mlm_probability": 0.15
    },
    "training_config": {
        "evaluation_strategy": "epoch",
        "save_strategy": "epoch",
        "logging_dir": "./logs",
        "num_training_epochs": 3,
        "per_device_train_batch_size": 64,
        "per_device_eval_batch_size": 64,
        "learning_rate": 2e-5,
        "save_total_limit": 1,
        "eval_steps": 500,
        "fp16": true
    }
}